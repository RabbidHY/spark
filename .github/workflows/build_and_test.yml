#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

name: Build and test

on:
  push:
    branches:
    - '**'
    - '!branch-*.*'
  schedule:
    # master
    - cron: '0 4 * * *'
    # branch-3.2
    - cron: '0 7 * * *'
    # PySpark coverage for master branch
    - cron: '0 10 * * *'

jobs:
  configure-jobs:
    name: Configure jobs
    runs-on: ubuntu-20.04
    outputs:
      branch: ${{ steps.set-outputs.outputs.branch }}
      type: ${{ steps.set-outputs.outputs.type }}
      envs: ${{ steps.set-outputs.outputs.envs }}
    steps:
    - name: Configure branch and additional environment variables
      id: set-outputs
      run: |
        if [ "${{ github.event.schedule }}" = "0 4 * * *" ]; then
          echo '::set-output name=branch::master'
          echo '::set-output name=type::scheduled'
          echo '::set-output name=envs::{"SCALA_PROFILE": "scala2.13"}'
        elif [ "${{ github.event.schedule }}" = "0 7 * * *" ]; then
          echo '::set-output name=branch::branch-3.2'
          echo '::set-output name=type::scheduled'
          echo '::set-output name=envs::{"SCALA_PROFILE": "scala2.13"}'
        elif [ "${{ github.event.schedule }}" = "0 10 * * *" ]; then
          echo '::set-output name=branch::master'
          echo '::set-output name=type::pyspark-coverage-scheduled'
          echo '::set-output name=envs::{"PYSPARK_CODECOV": "true"}'
        else
          echo '::set-output name=branch::master' # Default branch to run on. CHANGE here when a branch is cut out.
          echo '::set-output name=type::regular'
          echo '::set-output name=envs::{}'
        fi

  manual_test:
    needs: configure-jobs
    if: needs.configure-jobs.outputs.type == 'regular'
    name: Hadoop 2 build with SBT
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout Spark repository
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
          repository: apache/spark
          ref: master
      - name: Sync the current branch with the latest in Apache Spark
        if: github.repository != 'apache/spark'
        run: |
          git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit"
      - name: Cache Scala, SBT and Maven
        uses: actions/cache@v2
        with:
          path: |
            build/apache-maven-*
            build/scala-*
            build/*.jar
            ~/.sbt
          key: build-${{ hashFiles('**/pom.xml', 'project/build.properties', 'build/mvn', 'build/sbt', 'build/sbt-launch-lib.bash', 'build/spark-build-info') }}
          restore-keys: |
            build-
      - name: Cache Coursier local repository
        uses: actions/cache@v2
        with:
          path: ~/.cache/coursier
          key: hadoop-2-coursier-${{ hashFiles('**/pom.xml', '**/plugins.sbt') }}
          restore-keys: |
            hadoop-2-coursier-
      - name: Install Java 8
        uses: actions/setup-java@v1
        with:
          java-version: 8
      - name: Build with SBT
        run: |
          ./build/sbt "sql/testOnly *.DataFrameSuite"

  # Static analysis, and documentation build
  lint:
    needs: configure-jobs
    if: needs.configure-jobs.outputs.type == 'regular'
    name: Linters, licenses, dependencies and documentation generation
    runs-on: ubuntu-20.04
    env:
      LC_ALL: C.UTF-8
      LANG: C.UTF-8
      PYSPARK_DRIVER_PYTHON: python3.9
    container:
      image: dongjoon/apache-spark-github-action-image:20210602
    steps:
    - name: Checkout Spark repository
      uses: actions/checkout@v2
      with:
        fetch-depth: 0
        repository: apache/spark
        ref: master
    - name: Sync the current branch with the latest in Apache Spark
      if: github.repository != 'apache/spark'
      run: |
        git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit"
    # Cache local repositories. Note that GitHub Actions cache has a 2G limit.
    - name: Cache Scala, SBT and Maven
      uses: actions/cache@v2
      with:
        path: |
          build/apache-maven-*
          build/scala-*
          build/*.jar
          ~/.sbt
        key: build-${{ hashFiles('**/pom.xml', 'project/build.properties', 'build/mvn', 'build/sbt', 'build/sbt-launch-lib.bash', 'build/spark-build-info') }}
        restore-keys: |
          build-
    - name: Cache Coursier local repository
      uses: actions/cache@v2
      with:
        path: ~/.cache/coursier
        key: docs-coursier-${{ hashFiles('**/pom.xml', '**/plugins.sbt') }}
        restore-keys: |
          docs-coursier-
    - name: Cache Maven local repository
      uses: actions/cache@v2
      with:
        path: ~/.m2/repository
        key: docs-maven-${{ hashFiles('**/pom.xml') }}
        restore-keys: |
          docs-maven-
    - name: Install Python linter dependencies
      run: |
        # TODO(SPARK-32407): Sphinx 3.1+ does not correctly index nested classes.
        #   See also https://github.com/sphinx-doc/sphinx/issues/7551.
        # Jinja2 3.0.0+ causes error when building with Sphinx.
        #   See also https://issues.apache.org/jira/browse/SPARK-35375.
        python3.9 -m pip install flake8 pydata_sphinx_theme 'mypy==0.910' numpydoc 'jinja2<3.0.0' 'black==21.5b2'
    - name: Install R linter dependencies and SparkR
      run: |
        apt-get install -y libcurl4-openssl-dev libgit2-dev libssl-dev libxml2-dev
        Rscript -e "install.packages(c('devtools'), repos='https://cloud.r-project.org/')"
        Rscript -e "devtools::install_version('lintr', version='2.0.1', repos='https://cloud.r-project.org')"
        ./R/install-dev.sh
    - name: Instll JavaScript linter dependencies
      run: |
        apt update
        apt-get install -y nodejs npm
    - name: Install dependencies for documentation generation
      run: |
        # pandoc is required to generate PySpark APIs as well in nbsphinx.
        apt-get install -y libcurl4-openssl-dev pandoc
        # TODO(SPARK-32407): Sphinx 3.1+ does not correctly index nested classes.
        #   See also https://github.com/sphinx-doc/sphinx/issues/7551.
        # Jinja2 3.0.0+ causes error when building with Sphinx.
        #   See also https://issues.apache.org/jira/browse/SPARK-35375.
        python3.9 -m pip install 'sphinx<3.1.0' mkdocs numpy pydata_sphinx_theme ipython nbsphinx numpydoc 'jinja2<3.0.0'
        python3.9 -m pip install sphinx_plotly_directive 'pyarrow<5.0.0' pandas 'plotly>=4.8'
        apt-get update -y
        apt-get install -y ruby ruby-dev
        Rscript -e "install.packages(c('devtools', 'testthat', 'knitr', 'rmarkdown', 'roxygen2'), repos='https://cloud.r-project.org/')"
        gem install bundler
        cd docs
        bundle install
    - name: Scala linter
      run: ./dev/lint-scala
    - name: Java linter
      run: ./dev/lint-java
    - name: Python linter
      run: PYTHON_EXECUTABLE=python3.9 ./dev/lint-python
    - name: R linter
      run: ./dev/lint-r
    - name: JS linter
      run: ./dev/lint-js
    - name: License test
      run: ./dev/check-license
    - name: Dependencies test
      run: ./dev/test-dependencies.sh
    - name: Run documentation build
      run: |
        cd docs
        bundle exec jekyll build

